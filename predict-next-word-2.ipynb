{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Predict Next Word","metadata":{}},{"cell_type":"markdown","source":"## Prepare the Notebook","metadata":{}},{"cell_type":"code","source":"# Install Packages\n\n!pip install pytorch-lightning torchmetrics torchviz datasets -q|","metadata":{"execution":{"iopub.status.busy":"2023-07-05T00:58:03.570215Z","iopub.execute_input":"2023-07-05T00:58:03.570657Z","iopub.status.idle":"2023-07-05T00:58:04.526569Z","shell.execute_reply.started":"2023-07-05T00:58:03.570622Z","shell.execute_reply":"2023-07-05T00:58:04.525421Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/bin/bash: -c: line 2: syntax error: unexpected end of file\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import Packages\n\nfrom tqdm import tqdm\n\nimport re\nimport math\nimport pandas as pd\nfrom datasets import load_dataset\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\n\nnltk.download('stopwords')\n\nimport torch\nimport torchtext\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport torchmetrics\n\nimport warnings\n\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2023-07-05T01:11:46.583264Z","iopub.execute_input":"2023-07-05T01:11:46.584110Z","iopub.status.idle":"2023-07-05T01:11:46.597802Z","shell.execute_reply.started":"2023-07-05T01:11:46.584072Z","shell.execute_reply":"2023-07-05T01:11:46.596873Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import Dataset\n\ndataset = load_dataset('wikitext', 'wikitext-2-raw-v1')","metadata":{"execution":{"iopub.status.busy":"2023-07-05T00:58:15.383661Z","iopub.execute_input":"2023-07-05T00:58:15.389286Z","iopub.status.idle":"2023-07-05T00:58:15.888637Z","shell.execute_reply.started":"2023-07-05T00:58:15.389245Z","shell.execute_reply":"2023-07-05T00:58:15.887753Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53c326e34742461f8abaf00ede539b43"}},"metadata":{}}]},{"cell_type":"code","source":"dataset['train']['text'][9]","metadata":{"execution":{"iopub.status.busy":"2023-07-05T00:58:15.890439Z","iopub.execute_input":"2023-07-05T00:58:15.891316Z","iopub.status.idle":"2023-07-05T00:58:15.972148Z","shell.execute_reply.started":"2023-07-05T00:58:15.891282Z","shell.execute_reply":"2023-07-05T00:58:15.971129Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"\" As with previous Valkyira Chronicles games , Valkyria Chronicles III is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces . Stories are told through comic book @-@ like panels with animated character portraits , with characters speaking partially through voiced speech bubbles and partially through unvoiced text . The player progresses through a series of linear missions , gradually unlocked as maps that can be freely scanned through and replayed as they are unlocked . The route to each story location on the map varies depending on an individual player 's approach : when one option is selected , the other is sealed off to the player . Outside missions , the player characters rest in a camp , where units can be customized and character growth occurs . Alongside the main story missions are character @-@ specific sub missions relating to different squad members . After the game 's completion , additional episodes are unlocked , some of them having a higher difficulty than those found in the rest of the game . There are also love simulation elements related to the game 's two main heroines , although they take a very minor role . \\n\""},"metadata":{}}]},{"cell_type":"markdown","source":"## Data Analysis and Preprocessing","metadata":{}},{"cell_type":"code","source":"tokenizer = torchtext.data.utils.get_tokenizer(nltk.word_tokenize, language='basic-english')\nstemmer = SnowballStemmer('english')\nenglishStopwords = stopwords.words('english')","metadata":{"execution":{"iopub.status.busy":"2023-07-05T00:58:16.552473Z","iopub.execute_input":"2023-07-05T00:58:16.552866Z","iopub.status.idle":"2023-07-05T00:58:16.560024Z","shell.execute_reply.started":"2023-07-05T00:58:16.552837Z","shell.execute_reply":"2023-07-05T00:58:16.558941Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def clean(text):\n    text = text.lower()\n    text = re.sub('[^a-z A-Z 0-9-]+', '', text)\n    return [stemmer.stem(token).lower() for token in tokenizer(text) if token not in englishStopwords]","metadata":{"execution":{"iopub.status.busy":"2023-07-05T00:58:16.891376Z","iopub.execute_input":"2023-07-05T00:58:16.891735Z","iopub.status.idle":"2023-07-05T00:58:16.897014Z","shell.execute_reply.started":"2023-07-05T00:58:16.891700Z","shell.execute_reply":"2023-07-05T00:58:16.895909Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"smallDataset = dataset","metadata":{"execution":{"iopub.status.busy":"2023-07-05T00:58:17.142408Z","iopub.execute_input":"2023-07-05T00:58:17.142710Z","iopub.status.idle":"2023-07-05T00:58:17.147026Z","shell.execute_reply.started":"2023-07-05T00:58:17.142663Z","shell.execute_reply":"2023-07-05T00:58:17.145843Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"smallDataset","metadata":{"execution":{"iopub.status.busy":"2023-07-05T00:58:17.367019Z","iopub.execute_input":"2023-07-05T00:58:17.367311Z","iopub.status.idle":"2023-07-05T00:58:17.373552Z","shell.execute_reply.started":"2023-07-05T00:58:17.367286Z","shell.execute_reply":"2023-07-05T00:58:17.372330Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    test: Dataset({\n        features: ['text'],\n        num_rows: 4358\n    })\n    train: Dataset({\n        features: ['text'],\n        num_rows: 36718\n    })\n    validation: Dataset({\n        features: ['text'],\n        num_rows: 3760\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"tokenize_data = lambda example, clean: {'tokens': clean(example['text'])}  \ntokenized_dataset = smallDataset.map(tokenize_data, remove_columns=['text'], \nfn_kwargs={'clean': clean})\nprint(tokenized_dataset['train'][88]['tokens'])","metadata":{"execution":{"iopub.status.busy":"2023-07-05T00:58:17.624454Z","iopub.execute_input":"2023-07-05T00:58:17.625170Z","iopub.status.idle":"2023-07-05T00:59:13.693502Z","shell.execute_reply.started":"2023-07-05T00:58:17.625132Z","shell.execute_reply":"2023-07-05T00:59:13.692736Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4358 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"720533d2b8e841dcb9713bf56152f865"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/36718 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55c263e532d94f78b097bdb2b920837f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3760 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cd3a4c326f5447693a9004bf6eea57d"}},"metadata":{}},{"name":"stdout","text":"['ammunit', 'brought', 'rapid', 'prepar', 'use', 'laboratori', 'establish', 'littl', 'rock', 'arsenal', 'purpos', 'illustr', 'piti', 'scarciti', 'materi', 'countri', 'fact', 'may', 'state', 'found', 'necessari', 'use', 'public', 'document', 'state', 'librari', 'cartridg', 'paper', 'gunsmith', 'employ', 'conscript', 'tool', 'purchas', 'impress', 'repair', 'damag', 'gun', 'brought', 'equal', 'number', 'found', 'littl', 'rock', 'commenc', 'inspect', 'work', 'observ', 'spirit', 'men', 'decid', 'garrison', '500', 'strong', 'could', 'hold', 'fitch', 'would', 'lead', 'remaind', '-', '1500', '-', 'gen', 'l', 'rust', 'soon', 'shotgun', 'rifl', 'could', 'obtain', 'littl', 'rock', 'instead', 'pike', 'lanc', 'arm', 'two', 'day', 'elaps', 'chang', 'could', 'effect']\n","output_type":"stream"}]},{"cell_type":"code","source":"type(tokenized_dataset['train'][88]['tokens'])","metadata":{"execution":{"iopub.status.busy":"2023-07-05T00:59:13.695130Z","iopub.execute_input":"2023-07-05T00:59:13.695490Z","iopub.status.idle":"2023-07-05T00:59:13.703511Z","shell.execute_reply.started":"2023-07-05T00:59:13.695457Z","shell.execute_reply":"2023-07-05T00:59:13.702799Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"list"},"metadata":{}}]},{"cell_type":"code","source":"vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_dataset['train']['tokens'], \nmin_freq=3) \nvocab.insert_token('<unk>', 0)           \nvocab.insert_token('<eos>', 1)            \nvocab.set_default_index(vocab['<unk>'])   \nprint(len(vocab))                         \nprint(vocab.get_itos()[:10])            ","metadata":{"execution":{"iopub.status.busy":"2023-07-05T00:59:13.704492Z","iopub.execute_input":"2023-07-05T00:59:13.705901Z","iopub.status.idle":"2023-07-05T00:59:15.639793Z","shell.execute_reply.started":"2023-07-05T00:59:13.705868Z","shell.execute_reply":"2023-07-05T00:59:15.638751Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"21261\n['<unk>', '<eos>', '-', 'first', 'one', 'also', 'two', 'time', 'year', 'use']\n","output_type":"stream"}]},{"cell_type":"code","source":"\", \".join(vocab.get_itos()[:15])","metadata":{"execution":{"iopub.status.busy":"2023-07-05T00:59:15.642676Z","iopub.execute_input":"2023-07-05T00:59:15.643034Z","iopub.status.idle":"2023-07-05T00:59:15.651693Z","shell.execute_reply.started":"2023-07-05T00:59:15.643007Z","shell.execute_reply":"2023-07-05T00:59:15.650746Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'<unk>, <eos>, -, first, one, also, two, time, year, use, game, state, new, includ, song'"},"metadata":{}}]},{"cell_type":"code","source":"def getData(dataset, vocab, BATCH_SIZE):\n    data = []                                                   \n    for example in dataset:\n        if example['tokens']:                                      \n            tokens = example['tokens'].append('<eos>')             \n            tokens = [vocab[token] for token in example['tokens']] \n            data.extend(tokens)                                    \n    data = torch.LongTensor(data)                                 \n    numBatches = data.shape[0] // BATCH_SIZE \n    data = data[:numBatches * BATCH_SIZE]                       \n    data = data.view(BATCH_SIZE, numBatches)          \n    return data","metadata":{"execution":{"iopub.status.busy":"2023-07-05T00:59:15.653513Z","iopub.execute_input":"2023-07-05T00:59:15.654274Z","iopub.status.idle":"2023-07-05T00:59:15.661573Z","shell.execute_reply.started":"2023-07-05T00:59:15.654240Z","shell.execute_reply":"2023-07-05T00:59:15.660590Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 128\ntrain_data = getData(tokenized_dataset['train'], vocab, BATCH_SIZE)\nvalid_data = getData(tokenized_dataset['validation'], vocab, BATCH_SIZE)\ntest_data = getData(tokenized_dataset['test'], vocab, BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2023-07-05T00:59:15.666369Z","iopub.execute_input":"2023-07-05T00:59:15.667645Z","iopub.status.idle":"2023-07-05T00:59:21.215614Z","shell.execute_reply.started":"2023-07-05T00:59:15.667603Z","shell.execute_reply":"2023-07-05T00:59:21.214627Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"VOCAB_SIZE = len(vocab)","metadata":{"execution":{"iopub.status.busy":"2023-07-05T00:59:21.216914Z","iopub.execute_input":"2023-07-05T00:59:21.217285Z","iopub.status.idle":"2023-07-05T00:59:21.222847Z","shell.execute_reply.started":"2023-07-05T00:59:21.217251Z","shell.execute_reply":"2023-07-05T00:59:21.221710Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"execution":{"iopub.status.busy":"2023-07-05T00:59:21.224416Z","iopub.execute_input":"2023-07-05T00:59:21.225058Z","iopub.status.idle":"2023-07-05T00:59:21.302778Z","shell.execute_reply.started":"2023-07-05T00:59:21.225026Z","shell.execute_reply":"2023-07-05T00:59:21.301690Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"class ExtractTensor(nn.Module):\n    def forward(self, X):\n        output, hidden = X\n        return output[-1, :]","metadata":{"execution":{"iopub.status.busy":"2023-07-05T00:59:21.304592Z","iopub.execute_input":"2023-07-05T00:59:21.305085Z","iopub.status.idle":"2023-07-05T00:59:21.312653Z","shell.execute_reply.started":"2023-07-05T00:59:21.305050Z","shell.execute_reply":"2023-07-05T00:59:21.311710Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"vocabSize = len(vocab)\nembeddingDim = 1024\nhiddenDim = 1024\nnumLayer = 2\ndropoutRate = 0.65              \ntieWeights = True                  \nlr = 1e-3     \nbatchSize = 128","metadata":{"execution":{"iopub.status.busy":"2023-07-05T01:10:59.160829Z","iopub.execute_input":"2023-07-05T01:10:59.161202Z","iopub.status.idle":"2023-07-05T01:10:59.169207Z","shell.execute_reply.started":"2023-07-05T01:10:59.161172Z","shell.execute_reply":"2023-07-05T01:10:59.168160Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"class LSTM(nn.Module):\n    def __init__(self, vocabSize, embeddingDim, hiddenDim, numLayer, dropoutRate, tieWeights):\n        super().__init__()\n        self.numLayer = numLayer\n        self.hiddenDim = hiddenDim\n        self.embeddingDim = embeddingDim\n    \n        self.embedding = nn.Embedding(vocabSize, embeddingDim)\n        self.lstm = nn.LSTM(embeddingDim, hiddenDim, num_layers=numLayer, dropout=dropoutRate, batch_first=True)\n        self.dropout = nn.Dropout(dropoutRate)\n        self.linear = nn.Linear(hiddenDim, vocabSize)\n    \n        if tieWeights:\n            assert embeddingDim == hiddenDim, 'cannot tie' 'check dims'\n            self.embedding.weight = self.linear.weight\n        self.initWeights()\n    \n    def forward(self, src, hidden):\n        embedding = self.dropout(self.embedding(src))\n        output, hidden = self.lstm(embedding, hidden)\n        output = self.dropout(output)\n        prediction = self.linear(output)\n        return prediction\n    \n    def initWeights(self):\n        init_range_emb = 0.1\n        init_range_other = 1/math.sqrt(self.hiddenDim)\n        self.embedding.weight.data.uniform_(-init_range_emb, init_range_emb)\n        self.linear.weight.data.uniform_(-init_range_other, init_range_other)\n        self.linear.bias.data.zero_()\n        for i in range(self.numLayer):\n            self.lstm.all_weights[i][0] = torch.FloatTensor(self.embeddingDim,\n                    self.hiddenDim).uniform_(-init_range_other, init_range_other)\n            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hiddenDim,\n                    self.hiddenDim).uniform_(-init_range_other, init_range_other)\n\n    \n    def initHidden(self, batch_size, device):\n        hidden = torch.zeros(self.numLayer, batch_size, self.hiddenDim).to(device)\n        cell = torch.zeros(self.numLayer, batch_size, self.hiddenDim).to(device)\n        return hidden, cell\n    \n    def detachHidden(self, hidden):\n        hidden, cell = hidden\n        hidden = hidden.detach()\n        cell = cell.detach()\n        return hidden, cell\n\nmodel = LSTM(vocabSize, embeddingDim, hiddenDim, numLayer, dropoutRate, tieWeights).to(device)\noptimizer = optim.Adam(model.parameters(), lr=lr)\ncriterion = nn.CrossEntropyLoss()\nnum_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f'The model has {num_params:,} trainable parameters')","metadata":{"execution":{"iopub.status.busy":"2023-07-05T01:14:20.478699Z","iopub.execute_input":"2023-07-05T01:14:20.479077Z","iopub.status.idle":"2023-07-05T01:14:21.338961Z","shell.execute_reply.started":"2023-07-05T01:14:20.479048Z","shell.execute_reply":"2023-07-05T01:14:21.337886Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"The model has 38,586,125 trainable parameters\n","output_type":"stream"}]},{"cell_type":"code","source":"def getBatch(data, seqLen, numBatches, idx):\n    src = data[:, idx:idx+seqLen]\n    target = data[:, idx+1:idx+seqLen+1]\n    return src, target","metadata":{"execution":{"iopub.status.busy":"2023-07-05T00:59:24.741539Z","iopub.execute_input":"2023-07-05T00:59:24.742126Z","iopub.status.idle":"2023-07-05T00:59:24.747769Z","shell.execute_reply.started":"2023-07-05T00:59:24.742090Z","shell.execute_reply":"2023-07-05T00:59:24.746811Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def train(model, data, optimizer, criterion, batchSize, seqLen, clip, device):\n\n    epochLoss = 0\n    model.train()\n    numBatches = data.shape[-1]\n    data = data[:, :numBatches - (numBatches -1) % seqLen]\n    numBatches = data.shape[-1]\n\n    hidden = model.initHidden(batchSize, device)\n\n    for idx in tqdm(range(0, numBatches - 1, seqLen), desc='Training: ',leave=False):\n        optimizer.zero_grad()\n        hidden = model.detachHidden(hidden)\n\n        src, target = getBatch(data, seqLen, numBatches, idx)\n        src, target = src.to(device), target.to(device)\n        batch_size = src.shape[0]\n        prediction = model(src, hidden)\n\n        prediction = prediction.reshape(batchSize * seqLen, -1)\n        target = target.reshape(-1)\n        loss = criterion(prediction, target)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        epochLoss += loss.item() * seqLen\n    return epochLoss / numBatches","metadata":{"execution":{"iopub.status.busy":"2023-07-05T01:20:19.141055Z","iopub.execute_input":"2023-07-05T01:20:19.141458Z","iopub.status.idle":"2023-07-05T01:20:19.151867Z","shell.execute_reply.started":"2023-07-05T01:20:19.141428Z","shell.execute_reply":"2023-07-05T01:20:19.150846Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, data, criterion, batchSize, seqLen, device):\n\n    epochLoss = 0\n    model.eval()\n    numBatches = data.shape[-1]\n    data = data[:, :numBatches - (numBatches -1) % seqLen]\n    numBatches = data.shape[-1]\n\n    hidden = model.initHidden(batchSize, device)\n\n    with torch.no_grad():\n        for idx in range(0, numBatches - 1, seqLen):\n            hidden = model.detachHidden(hidden)\n            src, target = getBatch(data, seqLen, numBatches, idx)\n            src, target = src.to(device), target.to(device)\n            batch_size = src.shape[0]\n\n            prediction = model(src, hidden)\n            prediction = prediction.reshape(batchSize * seqLen, -1)\n            target = target.reshape(-1)\n\n\n            loss = criterion(prediction, target)\n            epochLoss += loss.item() * seqLen\n    return epochLoss / numBatches","metadata":{"execution":{"iopub.status.busy":"2023-07-05T01:20:19.999555Z","iopub.execute_input":"2023-07-05T01:20:20.002034Z","iopub.status.idle":"2023-07-05T01:20:20.010210Z","shell.execute_reply.started":"2023-07-05T01:20:20.001978Z","shell.execute_reply":"2023-07-05T01:20:20.009031Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"nEpochs = 10\nseqLen = 50\nclip = 0.25\nsaved = False\n\nlrScheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n\nif saved:\n    model.load_state_dict(torch.load('best-val-lstm_lm.pt',  map_location=device))\n    testLoss = evaluate(model, test_data, criterion, batchSize, seqLen, device)\n    print(f'Test Perplexity: {math.exp(testLoss):.3f}')\nelse:\n    bestValidLoss = float('inf')\n\n    for epoch in range(nEpochs):\n        trainLoss = train(model, train_data, optimizer, criterion, \n                    batchSize, seqLen, clip, device)\n        validLoss = evaluate(model, valid_data, criterion, batchSize, \n                    seqLen, device)\n        \n        lrScheduler.step(validLoss)\n\n        if validLoss < bestValidLoss:\n            bestValidLoss = validLoss\n            torch.save(model.state_dict(), 'best-val-lstm_lm.pt')\n\n        print(f'\\tTrain Perplexity: {math.exp(trainLoss):.3f}')\n        print(f'\\tValid Perplexity: {math.exp(validLoss):.3f}')","metadata":{"execution":{"iopub.status.busy":"2023-07-05T01:22:01.059601Z","iopub.execute_input":"2023-07-05T01:22:01.060361Z","iopub.status.idle":"2023-07-05T01:36:04.710944Z","shell.execute_reply.started":"2023-07-05T01:22:01.060326Z","shell.execute_reply":"2023-07-05T01:36:04.709717Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 3188.757\n\tValid Perplexity: 2449.526\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 2640.479\n\tValid Perplexity: 2027.794\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 2110.949\n\tValid Perplexity: 1749.192\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 1718.450\n\tValid Perplexity: 1509.963\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 1417.383\n\tValid Perplexity: 1378.071\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 1205.265\n\tValid Perplexity: 1281.776\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 1049.338\n\tValid Perplexity: 1219.801\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 927.260\n\tValid Perplexity: 1175.253\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 832.072\n\tValid Perplexity: 1142.512\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\tTrain Perplexity: 759.211\n\tValid Perplexity: 1114.313\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Evaluating","metadata":{}},{"cell_type":"code","source":"def generate(prompt, maxSeqLen, temperature, model, tokenizer, vocab, device, seed=None):\n    if seed is not None:\n        torch.manual_seed(seed)\n    model.eval()\n    tokens = clean(prompt)\n    indices = [vocab[t] for t in tokens]\n    batchSize = 1\n    hidden = model.initHidden(batchSize, device)\n    with torch.no_grad():\n        for i in range(maxSeqLen):\n            src = torch.LongTensor([indices]).to(device)\n            prediction = model(src, hidden)\n            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  \n            prediction = torch.multinomial(probs, num_samples=1).item()    \n            \n            while prediction == vocab['<unk>']:\n                prediction = torch.multinomial(probs, num_samples=1).item()\n\n            if prediction == vocab['<eos>']:\n                break\n\n            indices.append(prediction)\n\n    itos = vocab.get_itos()\n    tokens = [itos[i] for i in indices]\n    return tokens","metadata":{"execution":{"iopub.status.busy":"2023-07-05T01:36:36.365811Z","iopub.execute_input":"2023-07-05T01:36:36.366878Z","iopub.status.idle":"2023-07-05T01:36:36.376725Z","shell.execute_reply.started":"2023-07-05T01:36:36.366829Z","shell.execute_reply":"2023-07-05T01:36:36.375620Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"prompt = 'My name is'\nmaxSeqLen = 30\nseed = 0\n\ntemperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\nfor temperature in temperatures:\n    generation = generate(prompt, maxSeqLen, temperature, model, tokenizer, \n                          vocab, device, seed)\n    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')","metadata":{"execution":{"iopub.status.busy":"2023-07-05T01:37:49.401600Z","iopub.execute_input":"2023-07-05T01:37:49.402354Z","iopub.status.idle":"2023-07-05T01:37:49.555558Z","shell.execute_reply.started":"2023-07-05T01:37:49.402318Z","shell.execute_reply":"2023-07-05T01:37:49.554410Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"0.5\nname peopl\n\n0.7\nname robert davenport would describ reason issu dictat\n\n0.75\nname mention previous attempt liabil chess world peopl\n\n0.8\nname mention previous attempt liabil chess world peopl confer umpir investig umpir umpir\n\n1.0\nname mainstay 2501 previous attempt pessimist liabil chess film peopl park track investig movi peel\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Notes\n\n1. Don't use Stemmer\n2. Don't use Stopwords","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}