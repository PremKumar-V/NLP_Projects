{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Predict Next Word","metadata":{}},{"cell_type":"markdown","source":"## Prepare the Notebook","metadata":{}},{"cell_type":"code","source":"# Install Packages\n\n!pip install pytorch-lightning torchmetrics torchviz datasets -q","metadata":{"execution":{"iopub.status.busy":"2023-07-04T04:43:25.233409Z","iopub.execute_input":"2023-07-04T04:43:25.234147Z","iopub.status.idle":"2023-07-04T04:43:42.063283Z","shell.execute_reply.started":"2023-07-04T04:43:25.234113Z","shell.execute_reply":"2023-07-04T04:43:42.061882Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# Import Packages\n\nimport re\nimport pandas as pd\nfrom datasets import load_dataset\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\n\nnltk.download('stopwords')\n\nimport torch\nimport torchtext\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport pytorch_lightning as pl\n\nimport warnings\n\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2023-07-04T05:15:18.280684Z","iopub.execute_input":"2023-07-04T05:15:18.281058Z","iopub.status.idle":"2023-07-04T05:15:18.290043Z","shell.execute_reply.started":"2023-07-04T05:15:18.281028Z","shell.execute_reply":"2023-07-04T05:15:18.288814Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import Dataset\n\ndataset = load_dataset('wikitext', 'wikitext-2-raw-v1')","metadata":{"execution":{"iopub.status.busy":"2023-07-04T05:01:08.473970Z","iopub.execute_input":"2023-07-04T05:01:08.474343Z","iopub.status.idle":"2023-07-04T05:01:11.260444Z","shell.execute_reply.started":"2023-07-04T05:01:08.474301Z","shell.execute_reply":"2023-07-04T05:01:11.259490Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.03k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc49a75eaa754f2fb27702c9fac507fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/1.25k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bba61fa945f4b08935157b4db8279fa"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset wikitext/wikitext-2-raw-v1 (download: 4.50 MiB, generated: 12.90 MiB, post-processed: Unknown size, total: 17.40 MiB) to /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/4.72M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d42fa1b477be4b0fa6ad577cdf1c536c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset wikitext downloaded and prepared to /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f544a47bf374d8898a379e447d139a7"}},"metadata":{}}]},{"cell_type":"code","source":"dataset['train']['text'][9]","metadata":{"execution":{"iopub.status.busy":"2023-07-04T05:03:56.234039Z","iopub.execute_input":"2023-07-04T05:03:56.234473Z","iopub.status.idle":"2023-07-04T05:03:56.317663Z","shell.execute_reply.started":"2023-07-04T05:03:56.234438Z","shell.execute_reply":"2023-07-04T05:03:56.316631Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"\" As with previous Valkyira Chronicles games , Valkyria Chronicles III is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces . Stories are told through comic book @-@ like panels with animated character portraits , with characters speaking partially through voiced speech bubbles and partially through unvoiced text . The player progresses through a series of linear missions , gradually unlocked as maps that can be freely scanned through and replayed as they are unlocked . The route to each story location on the map varies depending on an individual player 's approach : when one option is selected , the other is sealed off to the player . Outside missions , the player characters rest in a camp , where units can be customized and character growth occurs . Alongside the main story missions are character @-@ specific sub missions relating to different squad members . After the game 's completion , additional episodes are unlocked , some of them having a higher difficulty than those found in the rest of the game . There are also love simulation elements related to the game 's two main heroines , although they take a very minor role . \\n\""},"metadata":{}}]},{"cell_type":"markdown","source":"## Data Analysis and Preprocessing","metadata":{}},{"cell_type":"code","source":"tokenizer = torchtext.data.utils.get_tokenizer(nltk.word_tokenize, language='basic-english')\nstemmer = SnowballStemmer('english')\nenglishStopwords = stopwords.words('english')","metadata":{"execution":{"iopub.status.busy":"2023-07-04T05:37:38.598665Z","iopub.execute_input":"2023-07-04T05:37:38.599063Z","iopub.status.idle":"2023-07-04T05:37:38.605055Z","shell.execute_reply.started":"2023-07-04T05:37:38.599031Z","shell.execute_reply":"2023-07-04T05:37:38.603912Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"def clean(text):\n    text = text.lower()\n    text = re.sub('[^a-z A-Z 0-9-]+', '', text)\n    return [stemmer.stem(token).lower() for token in tokenize(text) if token not in englishStopwords]","metadata":{"execution":{"iopub.status.busy":"2023-07-04T05:37:38.774674Z","iopub.execute_input":"2023-07-04T05:37:38.775321Z","iopub.status.idle":"2023-07-04T05:37:38.781796Z","shell.execute_reply.started":"2023-07-04T05:37:38.775282Z","shell.execute_reply":"2023-07-04T05:37:38.780581Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"smallDataset = dataset","metadata":{"execution":{"iopub.status.busy":"2023-07-04T05:44:32.994964Z","iopub.execute_input":"2023-07-04T05:44:32.995363Z","iopub.status.idle":"2023-07-04T05:44:33.000205Z","shell.execute_reply.started":"2023-07-04T05:44:32.995312Z","shell.execute_reply":"2023-07-04T05:44:32.999109Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"smallDataset","metadata":{"execution":{"iopub.status.busy":"2023-07-04T05:44:33.202385Z","iopub.execute_input":"2023-07-04T05:44:33.203234Z","iopub.status.idle":"2023-07-04T05:44:33.209673Z","shell.execute_reply.started":"2023-07-04T05:44:33.203190Z","shell.execute_reply":"2023-07-04T05:44:33.208597Z"},"trusted":true},"execution_count":78,"outputs":[{"execution_count":78,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    test: Dataset({\n        features: ['text'],\n        num_rows: 4358\n    })\n    train: Dataset({\n        features: ['text'],\n        num_rows: 36718\n    })\n    validation: Dataset({\n        features: ['text'],\n        num_rows: 3760\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"tokenize_data = lambda example, clean: {'tokens': clean(example['text'])}  \ntokenized_dataset = smallDataset.map(tokenize_data, remove_columns=['text'], \nfn_kwargs={'clean': clean})\nprint(tokenized_dataset['train'][88]['tokens'])","metadata":{"execution":{"iopub.status.busy":"2023-07-04T05:44:46.161007Z","iopub.execute_input":"2023-07-04T05:44:46.161414Z","iopub.status.idle":"2023-07-04T05:45:44.879467Z","shell.execute_reply.started":"2023-07-04T05:44:46.161377Z","shell.execute_reply":"2023-07-04T05:45:44.878625Z"},"trusted":true},"execution_count":79,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4358 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"038ef5405061405eb5ebf175f24e98cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/36718 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e666d85b3ae1445aa62ff860dae66516"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3760 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc5f3925f3e74abea1b87ed7d160d1f5"}},"metadata":{}},{"name":"stdout","text":"['ammunit', 'brought', 'rapid', 'prepar', 'use', 'laboratori', 'establish', 'littl', 'rock', 'arsenal', 'purpos', 'illustr', 'piti', 'scarciti', 'materi', 'countri', 'fact', 'may', 'state', 'found', 'necessari', 'use', 'public', 'document', 'state', 'librari', 'cartridg', 'paper', 'gunsmith', 'employ', 'conscript', 'tool', 'purchas', 'impress', 'repair', 'damag', 'gun', 'brought', 'equal', 'number', 'found', 'littl', 'rock', 'commenc', 'inspect', 'work', 'observ', 'spirit', 'men', 'decid', 'garrison', '500', 'strong', 'could', 'hold', 'fitch', 'would', 'lead', 'remaind', '-', '1500', '-', 'gen', 'l', 'rust', 'soon', 'shotgun', 'rifl', 'could', 'obtain', 'littl', 'rock', 'instead', 'pike', 'lanc', 'arm', 'two', 'day', 'elaps', 'chang', 'could', 'effect']\n","output_type":"stream"}]},{"cell_type":"code","source":"type(tokenized_dataset['train'][88]['tokens'])","metadata":{"execution":{"iopub.status.busy":"2023-07-04T05:45:44.881154Z","iopub.execute_input":"2023-07-04T05:45:44.881533Z","iopub.status.idle":"2023-07-04T05:45:44.890911Z","shell.execute_reply.started":"2023-07-04T05:45:44.881498Z","shell.execute_reply":"2023-07-04T05:45:44.890067Z"},"trusted":true},"execution_count":80,"outputs":[{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"list"},"metadata":{}}]},{"cell_type":"markdown","source":"##### ","metadata":{}},{"cell_type":"code","source":"vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_dataset['train']['tokens'], \nmin_freq=3) \nvocab.insert_token('<unk>', 0)           \nvocab.insert_token('<eos>', 1)            \nvocab.set_default_index(vocab['<unk>'])   \nprint(len(vocab))                         \nprint(vocab.get_itos()[:10])            ","metadata":{"execution":{"iopub.status.busy":"2023-07-04T05:45:54.591067Z","iopub.execute_input":"2023-07-04T05:45:54.591483Z","iopub.status.idle":"2023-07-04T05:45:56.702819Z","shell.execute_reply.started":"2023-07-04T05:45:54.591449Z","shell.execute_reply":"2023-07-04T05:45:56.701805Z"},"trusted":true},"execution_count":81,"outputs":[{"name":"stdout","text":"21261\n['<unk>', '<eos>', '-', 'first', 'one', 'also', 'two', 'time', 'year', 'use']\n","output_type":"stream"}]},{"cell_type":"code","source":"\", \".join(vocab.get_itos()[:15])","metadata":{"execution":{"iopub.status.busy":"2023-07-04T05:46:42.143959Z","iopub.execute_input":"2023-07-04T05:46:42.144342Z","iopub.status.idle":"2023-07-04T05:46:42.154725Z","shell.execute_reply.started":"2023-07-04T05:46:42.144296Z","shell.execute_reply":"2023-07-04T05:46:42.153476Z"},"trusted":true},"execution_count":84,"outputs":[{"execution_count":84,"output_type":"execute_result","data":{"text/plain":"'<unk>, <eos>, -, first, one, also, two, time, year, use, game, state, new, includ, song'"},"metadata":{}}]},{"cell_type":"code","source":"def getData(dataset, vocab, BATCH_SIZE):\n    data = []                                                   \n    for example in dataset:\n        if example['tokens']:                                      \n            tokens = example['tokens'].append('<eos>')             \n            tokens = [vocab[token] for token in example['tokens']] \n            data.extend(tokens)                                    \n    data = torch.LongTensor(data)                                 \n    numBatches = data.shape[0] // BATCH_SIZE \n    data = data[:numBatches * BATCH_SIZE]                       \n    data = data.view(BATCH_SIZE, numBatches)          \n    return data","metadata":{"execution":{"iopub.status.busy":"2023-07-04T05:52:43.599923Z","iopub.execute_input":"2023-07-04T05:52:43.600290Z","iopub.status.idle":"2023-07-04T05:52:43.607567Z","shell.execute_reply.started":"2023-07-04T05:52:43.600261Z","shell.execute_reply":"2023-07-04T05:52:43.606289Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 128\ntrain_data = getData(tokenized_dataset['train'], vocab, BATCH_SIZE)\nvalid_data = getData(tokenized_dataset['validation'], vocab, BATCH_SIZE)\ntest_data = getData(tokenized_dataset['test'], vocab, BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2023-07-04T05:52:43.793322Z","iopub.execute_input":"2023-07-04T05:52:43.793717Z","iopub.status.idle":"2023-07-04T05:52:49.485725Z","shell.execute_reply.started":"2023-07-04T05:52:43.793688Z","shell.execute_reply":"2023-07-04T05:52:49.484653Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"VOCAB_SIZE = len(vocab)","metadata":{"execution":{"iopub.status.busy":"2023-07-04T06:37:07.314053Z","iopub.execute_input":"2023-07-04T06:37:07.314770Z","iopub.status.idle":"2023-07-04T06:37:07.320547Z","shell.execute_reply.started":"2023-07-04T06:37:07.314735Z","shell.execute_reply":"2023-07-04T06:37:07.319506Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"device = ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ExtractTensor(nn.Module):\n    def forward(self, X):\n        output, hidden = X\n        return output[-1, :]","metadata":{"execution":{"iopub.status.busy":"2023-07-04T06:44:18.908438Z","iopub.execute_input":"2023-07-04T06:44:18.908838Z","iopub.status.idle":"2023-07-04T06:44:18.914524Z","shell.execute_reply.started":"2023-07-04T06:44:18.908805Z","shell.execute_reply":"2023-07-04T06:44:18.913300Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"class LSTM(nn.Module):\n    def __init__(self, vocabSize, embeddingDim, hiddenDim, numLayer, dropoutRate, tieWeights):\n        super().__init__()\n        self.numLayer = numLayer\n        self.hiddenDim = hiddenDim\n        self.embeddingDim = embeddingDim\n    \n        self.embedding = nn.Embedding(vocabSize, embeddingDim)\n        self.lstm = nn.LSTm(embeddingDim, hiddenDim, num_layers = numLayer, dropout, dropoutRate, batch_first = True)\n        self.dropout = nn.Dropout(dropoutRate)\n        self.linear = nn.Linear(hiddenDim, vocab_size)\n    \n        if tieWeights:\n            assert embeddingDim == hiddenDim, 'cannot tie', 'check dims'\n            self.embedding.weight = self.linear.weight\n        self.initWeights()\n    \n    def forward(self, src, hidden):\n        ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate,\n                tie_weights):\n\n        super().__init__()\n        self.num_layers = num_layers\n        self.hidden_dim = hidden_dim\n        self.embedding_dim = embedding_dim\n\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers,\n                    dropout=dropout_rate, batch_first=True)\n        self.dropout = nn.Dropout(dropout_rate)\n        self.fc = nn.Linear(hidden_dim, vocab_size)\n\n        if tie_weights:\n            assert embedding_dim == hidden_dim, 'cannot tie, check dims'\n            self.embedding.weight = self.fc.weight\n        self.init_weights()\n\n    def forward(self, src, hidden):\n        embedding = self.dropout(self.embedding(src))\n        output, hidden = self.lstm(embedding, hidden)\n        output = self.dropout(output)\n        prediction = self.fc(output)\n        return prediction, hidden\n\n    def init_weights(self):\n        init_range_emb = 0.1\n        init_range_other = 1/math.sqrt(self.hidden_dim)\n        self.embedding.weight.data.uniform_(-init_range_emb, init_range_emb)\n        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n        self.fc.bias.data.zero_()\n        for i in range(self.num_layers):\n            self.lstm.all_weights[i][0] = torch.FloatTensor(self.embedding_dim,\n                    self.hidden_dim).uniform_(-init_range_other, init_range_other)\n            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hidden_dim,\n                    self.hidden_dim).uniform_(-init_range_other, init_range_other)\n\n    def init_hidden(self, batch_size, device):\n        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n        cell = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n        return hidden, cell\n\n    def detach_hidden(self, hidden):\n        hidden, cell = hidden\n        hidden = hidden.detach()\n        cell = cell.detach()\n        return hidden, cell","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampleModel = nn.Sequential(\n    nn.Embedding(VOCAB_SIZE, 1024),\n    nn.LSTM(1024, 1024, num_layers = 2, batch_first = True),\n    ExtractTensor()\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-04T06:41:06.350411Z","iopub.execute_input":"2023-07-04T06:41:06.350781Z","iopub.status.idle":"2023-07-04T06:41:06.760156Z","shell.execute_reply.started":"2023-07-04T06:41:06.350751Z","shell.execute_reply":"2023-07-04T06:41:06.759129Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"for value in train_data:\n    print(value.size())\n    output = sampleModel(value)\n    print(output.size())\n    break","metadata":{"execution":{"iopub.status.busy":"2023-07-04T06:41:06.813199Z","iopub.execute_input":"2023-07-04T06:41:06.813873Z","iopub.status.idle":"2023-07-04T06:41:21.507561Z","shell.execute_reply.started":"2023-07-04T06:41:06.813839Z","shell.execute_reply":"2023-07-04T06:41:21.506397Z"},"trusted":true},"execution_count":116,"outputs":[{"name":"stdout","text":"torch.Size([8422])\ntorch.Size([1024])\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}